{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Writing Data\n",
    "\n",
    "Note: Parts of this notebook will only run on a Windows machine with Microsoft SQL Server installed. You can install the developer edition for free from [here](https://www.microsoft.com/en-us/sql-server/sql-server-downloads).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandas\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` can easily read in data from a variety of sources, including CSV files, Excel files, SQL databases, and JSON files. In this section, we will learn how to use `pandas` to extract data from external sources. First of all, we need to import the `pandas` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Files\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV (comma-separated values) files are a common format for storing data. You can read a CSV file using the `read_csv` function. The function accepts both a file path and a URL as input. The documentation of the `read_csv` function is available at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = pd.read_csv(\"../../data/AB_NYC_2019_unclean.csv\")\n",
    "\n",
    "csv_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing files is as easy as reading them. You can write a CSV file using the `to_csv` function. The documentation of the `to_csv` function is available at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data.to_csv(\"../../output/AB_NYC_2019_unclean.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Files\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON (JavaScript Object Notation) is a popular data format that stores data as key-value pairs. You can read a JSON file using the `read_json` function. Like `read_csv`, the function accepts both a file path and a URL as input. The documentation of the `read_json` function is available at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = pd.read_json(\"../../data/AB_NYC_2019_unclean.json\")\n",
    "\n",
    "json_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, writing JSON files is a very simple process. You can write a JSON file using the `to_json` function. The documentation of the `to_json` function is available at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_json.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data.to_json(\"../../output/AB_NYC_2019_unclean.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel Files\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` can read and write Excel files, but there are some important points to be aware of:\n",
    "\n",
    "- It can only import data, not images or macros.\n",
    "- By default, it imports the first sheet of the file.\n",
    "- It does not import formulae, formatting, merged cells, and so on.\n",
    "- You may need to install additional packages, depending on the version of Excel you are using. For example, if you are using Excel 2016, you need to install `openpyxl` to read `.xlsx` files.\n",
    "  - You can install it using `pip install openpyxl`.\n",
    "\n",
    "`read_excel` supports `xls`, `xlsx`, `xlsm`, `xlsb`, `odf`, `ods` and `odt` file extensions read from a local filesystem or URL. To function also has the ability to read a single sheet or a list of sheets.\n",
    "\n",
    "See the documentation of the `read_excel` function at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html for further information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = pd.read_excel(\"../../data/AB_NYC_2019_unclean.xlsx\")\n",
    "\n",
    "excel_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing Excel files is a little more complicated than reading them. You can use the `to_excel` function to write a `DataFrame` to an Excel file as a single sheet. However, you cannot write multiple `DataFrame`s to a single Excel file this way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data.to_excel(\"../../output/AB_NYC_2019_unclean.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to write multiple `DataFrame`s to a single Excel file, you need to use the `ExcelWriter` class. The documentation of the `ExcelWriter` class is available at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.ExcelWriter.html. The following example shows how to write multiple `DataFrame`s to a single Excel file. Be aware, this method can take a long time to run if you have a lot of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter\n",
    "\n",
    "clean_data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/AB_NYC_2019.csv\"\n",
    ")\n",
    "\n",
    "with ExcelWriter(\"../../output/AB_NYC_2019.xlsx\") as writer:\n",
    "    excel_data.to_excel(writer, sheet_name=\"raw_data\", index=False)\n",
    "    clean_data.to_excel(writer, sheet_name=\"clean_data\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Databases\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` can read and write data from a variety of SQL databases, including MySQL, PostgreSQL, SQLite, and Microsoft SQL Server. We will be focusing on Microsoft SQL Server in this section.\n",
    "\n",
    "First of all, lets create a database connection to a Microsoft SQL Server database. We will use the `sqlalchemy` and `pyodbc` library to create the connection. You can install `pyodbc` using `pip install pyodbc`. The documentation of the `create_engine` function is available at https://docs.sqlalchemy.org/en/20/core/engines.html. Note: make sure to enable TCP/IP connections in SQL Server Configuration Manager and to allow SQL Server to accept remote connections.\n",
    "\n",
    "Note: If you are creating a Microsoft SQL Server database from scratch/installing for the first time, make sure to create a user with a password and to enable SQL Server authentication. You can do this by right-clicking on the server in SQL Server Management Studio and selecting Properties. Then, go to the Security tab and select SQL Server and Windows Authentication mode. You can then create a new user by right-clicking on the Security folder and selecting New > Login. Make sure to give the user the db_owner role for the database you are connecting to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "server = \"localhost\"\n",
    "database = \"nyc_airbnb\"\n",
    "username = \"user\"\n",
    "password = \"Pass123!\"\n",
    "driver = \"SQL+Server\"  # or \"ODBC+Driver+18+for+SQL+Server\" depending on your configuration\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver={driver}\"\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    engine.connect()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise Exception(\"Unable to connect to database\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a table in the database. We will do this using the `execute` method of the `engine` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"DROP TABLE data\"))\n",
    "    conn.execute(\n",
    "        text(\n",
    "            \"\"\"\n",
    "                CREATE TABLE data (\n",
    "                    id INTEGER PRIMARY KEY,\n",
    "                    name TEXT,\n",
    "                    host_id INTEGER,\n",
    "                    host_name TEXT,\n",
    "                    neighbourhood_group TEXT,\n",
    "                    neighbourhood TEXT,\n",
    "                    latitude REAL,\n",
    "                    longitude REAL,\n",
    "                    room_type TEXT,\n",
    "                    price INTEGER,\n",
    "                    minimum_nights INTEGER,\n",
    "                    number_of_reviews INTEGER,\n",
    "                    last_review TEXT,\n",
    "                    reviews_per_month REAL,\n",
    "                    calculated_host_listings_count INTEGER,\n",
    "                    availability_365 INTEGER\n",
    "                )\n",
    "            \"\"\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets import `pandas` and import the NYC Airbnb data into a `DataFrame`. We can then commit this data to the database using the `to_sql` function. The documentation of the `to_sql` function is available at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html. If we were to use `to_sql` without initially creating the table, `pandas` would create the table for us automatically. However, we would have no control over the data types of the columns. By creating the table manually, we can ensure that the data types are correct. In particular, you cannot specify a primary key and non-nullable datatypes using `to_sql`. This can potentially cause problems down the line, especially if you are using the database for production purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "clean_data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/AB_NYC_2019.csv\"\n",
    ")\n",
    "\n",
    "clean_data.to_sql(\"data\", con=engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the SQL Database is populated with data, we can query it into a `pandas` `DataFrame` using the `read_sql` function. The documentation of the `read_sql` function is available at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_data = pd.read_sql(\"SELECT * FROM data\", con=engine)\n",
    "\n",
    "sql_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a SQLAlchemy `Select` statement instead of a string query to read in the database. It is a little more complicated, but it allows us to leverage the full combined power of SQL and Python. First of all, we need to map the tables we're interested in to Python classes. We can do this manually, but it is much easier to use the `automap_base` function. Let's see how it would look if we were to do it manually:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "from sqlalchemy import Column, Integer, String, Float\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class DataManual(Base):\n",
    "    __tablename__ = \"data\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String)\n",
    "    host_id = Column(Integer)\n",
    "    host_name = Column(String)\n",
    "    neighbourhood_group = Column(String)\n",
    "    neighbourhood = Column(String)\n",
    "    latitude = Column(Float)\n",
    "    longitude = Column(Float)\n",
    "    room_type = Column(String)\n",
    "    price = Column(Integer)\n",
    "    minimum_nights = Column(Integer)\n",
    "    number_of_reviews = Column(Integer)\n",
    "    last_review = Column(String)\n",
    "    reviews_per_month = Column(Float)\n",
    "    calculated_host_listings_count = Column(Integer)\n",
    "    availability_365 = Column(Integer)\n",
    "\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "query = session.query(DataManual)\n",
    "\n",
    "sql_data = pd.read_sql(query.statement, con=engine)\n",
    "\n",
    "sql_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping every table manually in a database would be in extremely tedious process. Fortunately, we can use the `automap_base` function to automatically map all the tables in a database. Note: Tables without a primary key will not be mapped. The documentation of the `automap_base` function is available at https://docs.sqlalchemy.org/en/20/orm/extensions/automap.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.automap import automap_base\n",
    "\n",
    "# reflect the tables\n",
    "Base = automap_base()\n",
    "Base.prepare(autoload_with=engine)\n",
    "\n",
    "# mapped classes are now created with names by default\n",
    "# matching that of the table name.\n",
    "Data = Base.classes.data\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "query = session.query(Data)\n",
    "\n",
    "sql_data = pd.read_sql(query.statement, con=engine)\n",
    "\n",
    "sql_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use `pyodbc` directly to connect to the database instead of using `sqlalchemy`. However, this method is not recommended as `pandas` does not natively support `pyodbc` and it is not as flexible as using `sqlalchemy`. You can find the documentation for `pyodbc` at https://github.com/mkleehammer/pyodbc/wiki.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "server = \"localhost\"\n",
    "database = \"nyc_airbnb\"\n",
    "username = \"user\"\n",
    "password = \"Pass123!\"\n",
    "driver = \"{SQL Server}\"\n",
    "conn_str = f\"Driver={driver};Server={server};Database={database};Uid={username};Pwd={password};TrustServerCertificate=yes;Connection Timeout=30;\"\n",
    "\n",
    "\n",
    "try:\n",
    "    pyodbc.connect(conn_str)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise Exception(\"Unable to connect to database\")\n",
    "\n",
    "with pyodbc.connect(conn_str) as cnxn:\n",
    "    # select from SQL table to insert in dataframe.\n",
    "    query = \"SELECT * FROM data\"\n",
    "    df = pd.read_sql(query, cnxn)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only covered a small number of `pandas` IO tools in this section. For more information, see the documentation of the `pandas` library at https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest APIs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REST (Representational State Transfer) API is a web-based service that uses HTTP methods to get, send, delete, or update data. Many online platforms provide REST APIs to interact with their data, and in most cases, this data is returned in JSON format.\n",
    "\n",
    "To work with REST APIs, you will need a package for sending HTTP requests. Python's requests package is a great choice for this task. You can install it using pip:\n",
    "\n",
    "`pip install requests`\n",
    "\n",
    "Let's assume we have a REST API that returns a JSON response. In our example, we will use a public API that provides information about users: https://jsonplaceholder.typicode.com/users.\n",
    "\n",
    "Here is how you can read this data into a pandas DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Define the API endpoint\n",
    "url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "\n",
    "# Send a GET request to the REST API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check that the GET request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response to a Python dictionary\n",
    "    data = response.json()\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will output a DataFrame where each row corresponds to a user, and columns correspond to user attributes (id, name, username, etc.). If you want to read only a specific attribute from the API response, you can do it by selecting this attribute when creating the DataFrame. Let's say you are interested in the names and email addresses of the users only:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\"name\", \"email\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be aware that not all APIs are public and some require authentication. The procedure to authenticate will depend on the specific API, so always refer to the API's documentation for instructions. Also, always be aware of the API's rate limits. If you send too many requests in a short period of time, the server might block your IP address. To avoid this, you can add pauses between your requests using the `time.sleep()` function. Finally, keep in mind that API responses can be large, and downloading and processing them can take a lot of time. Consider filtering the data on the server side (if the API supports this) or processing the data in chunks.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is the process of extracting data from websites. Python, with its rich ecosystem of libraries, is a popular choice for web scraping tasks. In this tutorial, we'll cover the basics of web scraping using Python and two popular libraries: Requests and Beautiful Soup.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the Libraries\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, you need to install Requests and Beautiful Soup libraries. To do this, run the following command:\n",
    "\n",
    "```bash\n",
    "pip install requests beautifulsoup4\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching a Web Page\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to fetch the web page's content. We'll use the Requests library for this purpose. In this tutorial, we'll use the [Wikipedia page for \"List of state and union territory capitals in India\"](https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India) as an example. The following code fetches the content of the web page and stores it in the `page_content` variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Page successfully fetched!\", end=\"\\n\\n\")\n",
    "    page_content = response.text\n",
    "    print(f\"Object type: {type(page_content)}\", end=\"\\n\\n\")\n",
    "    print(\"Page content:\", end=\"\\n\\n\")\n",
    "    print(page_content)\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: unable to fetch the page.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Web Page\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the HTML content, we need to parse it to navigate and extract the data. Beautiful Soup will help us with this task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "\n",
    "print(f\"Object type: {type(soup)}\", end=\"\\n\\n\")\n",
    "print(\"Page content:\", end=\"\\n\\n\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing the HTML, we can now locate and extract the desired information using Beautiful Soup. Let's say we want to extract all the headings (h1, h2, and h3 tags) from the page. We can do this using the `find_all` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = soup.find_all([\"h1\", \"h2\", \"h3\"])\n",
    "\n",
    "for heading in headings:\n",
    "    print(heading.get_text())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a specific heading by using the `find` method and passing the tag id as a keyword argument:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_heading = soup.find(id=\"List\")\n",
    "\n",
    "print(specific_heading.get_text())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Tables to Pandas DataFrames\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page contains tables with of states and union territories. We can extract the tables using the `find_all` method of Beautiful Soup and convert them to a list of Pandas DataFrames using the `read_html` method of Pandas.\n",
    "\n",
    "Note you may need to install the `lxml` and `html5lib` libraries to use the `read_html` method. You can install them using the following command:\n",
    "\n",
    "```bash\n",
    "pip install lxml html5lib\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "df_list = pd.read_html(str(tables))\n",
    "\n",
    "for df in df_list:\n",
    "    display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 01: Extracting Books Data Into A Database\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a starting point you can use the library management system you built in the previous tutorial and ingest books data from external sources to populate your database. If you haven't built the library management system yet, you can use the solution code from the previous tutorial (check the GitHub repository) as a starting point.\n",
    "\n",
    "For this project however, you can use any external data source you want and create an appropriate database schema to store the data. The data sets below are merely suggestions. You can use any data source you want as long as it is in a format that can be ingested into a database. You can use CSV files, REST APIs, or web scraping to get the data. You can use any database you want, but we recommend using Microsoft SQL Server or SQLite for this project.\n",
    "\n",
    "For CSV files, here are some datasets you can use:\n",
    "\n",
    "- [Books Dataset](https://www.kaggle.com/datasets/saurabhbagchi/books-dataset)\n",
    "- [Goodreads-books](https://www.kaggle.com/jealousleopard/goodreadsbooks)\n",
    "- [goodbooks-10k](https://www.kaggle.com/zygmunt/goodbooks-10k)\n",
    "- [Book Recommender: Collaborative Filtering, Shiny](https://www.kaggle.com/philippsp/book-recommender-collaborative-filtering-shiny)\n",
    "- [Amazon Top 50 Bestselling Books 2009 - 2019](https://www.kaggle.com/sootersaalu/amazon-top-50-bestselling-books-2009-2019)\n",
    "\n",
    "If you are using a REST API, here are some APIs you can use:\n",
    "\n",
    "- [Open Library Books API](https://openlibrary.org/developers/api)\n",
    "- [Google Books APIs](https://developers.google.com/books)\n",
    "\n",
    "If you are using web scraping. Here are some websites you can use:\n",
    "\n",
    "- [Books To Scrape](http://books.toscrape.com/)\n",
    "- [Project Gutenberg](https://www.gutenberg.org/)\n",
    "- [Open Library](https://openlibrary.org/)\n",
    "\n",
    "If you are web scraping, to get you started, here's a code snippet that fetches and prints the titles of all the books on the first page of the Books To Scrape website:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://books.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Page successfully fetched!\")\n",
    "    page_content = response.text\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: unable to fetch the page.\")\n",
    "\n",
    "soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "\n",
    "books = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "\n",
    "for book in books:\n",
    "    title = book.find(\"h3\").find(\"a\")[\"title\"]\n",
    "\n",
    "    print(f\"Title: {title}\")\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to inspect the HTML of the website to figure out the tags and attributes to use for extracting the data. You can use the \"Inspect\" option in your browser's right-click menu to open the developer tools. You can also use the \"View Page Source\" option to view the HTML source code of the page.\n",
    "\n",
    "You can further customize your code to navigate through multiple pages and extract more data. You could try to extract the data into a pandas DataFrame to perform further analyses and manipulations before committing to your database. The possibilities are endless!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 02: Data Exploration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a sample AdventureWorks SQL database on Microsoft Azure that you can use for this project. Use `sqlalchemy` and `pyodbc` to connect to the database and read the data into a pandas DataFrame. You can use the following code snippet to connect to the database:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "server = \"rdp-phsa.database.windows.net\"\n",
    "database = \"db\"\n",
    "username = \"reader\"\n",
    "password = \"Temppassword123\"\n",
    "driver = \"ODBC+Driver+18+for+SQL+Server\"\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver={driver}\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    engine.connect()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise Exception(\"Unable to connect to database\")\n",
    "\n",
    "\n",
    "query = \"SELECT * FROM SalesLT.SalesOrderDetail\"\n",
    "df = pd.read_sql(query, con=engine)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data from each table into separate DataFrames and explore and manipulate the data using pandas. If you wish, you can also use the `sqlalchemy` library to perform SQL queries on the database and read the results into a DataFrame. Try generating some visualizations using the `matplotlib` library. You can also use the `seaborn` library to generate more advanced visualizations. Alternatively, you can output the data to a CSV file and use a tool like Microsoft Excel or Power BI to generate visualizations.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the following resources for more information on importing data into Python:\n",
    "\n",
    "- [pandas: How to Read and Write Files](https://realpython.com/pandas-read-write-fi)\n",
    "- [pandas Documentation: IO Tools](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)\n",
    "- [REST APIs with Python](https://realpython.com/api-integration-in-python/)\n",
    "- [Web Scraping with Python: A Beginner's Guide](https://realpython.com/python-web-scraping-practical-introduction/)\n",
    "- [Beautiful Soup: Build a Web Scraper With Python](https://realpython.com/beautiful-soup-web-scraper-python/)\n",
    "- [Python Web Scraping Tutorial](https://www.freecodecamp.org/news/how-to-scrape-websites-with-python-2/)\n",
    "- [Requests Documentation](https://requests.readthedocs.io/en/master/)\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
